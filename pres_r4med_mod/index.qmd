---
title: "R for Medical Research"
subtitle: "Basic Modeling"
author: "Matt Broerman"
date: today
format:
  revealjs: 
    slide-number: true
    preview-links: auto
    theme: [simple, custom.scss]
execute:
  echo: true
---

```{r include=FALSE}
library(tidyverse)
theme_set(theme_minimal())
```



## Overview

Today we will explore the basics of modeling with a toy dataset and show how to compare multiple models in a nice format. 

We will use all of the skills from previous sessions: horizonal and vertical verbs, reshape, joins, tables, and plots. 


# Load Data

## Trial data

Trial toy dataset.

:::: columns

::: column
```{r}
set.seed(125)
id <- paste0("id", seq(1:6))
id2 <- rep(id, times = 2)
grp <- rep(c("trt", "crl"), 
           times = 6)
vst <- rep(c("vst1", "vst2"), 
           each = 6)
res <- c(rnorm(3, 10, 1), 
         rnorm(3, 10, 1), 
         rnorm(3, 12, 1), 
         rnorm(3, 17, 1))

trial <- tibble(id2, vst, grp) |> 
  arrange(vst, grp) |> 
  mutate(res = res) |> 
  arrange(vst, id2)
```

:::

::: column

```{r echo=FALSE}
trial
```

::: 
::::

## Notice

:::: columns

::: column

```{r eval=FALSE}
res <- c(rnorm(3, 10, 1), 
         rnorm(3, 10, 1), 
         rnorm(3, 12, 1), 
         rnorm(3, 17, 1))
```

```{r echo=FALSE}
trial |> 
  arrange(vst, id2, grp)
```

:::

::: column

Since this is a toy dataset, we know the "data generating process". All subjects start at visit one with a test result drawn from $N(10, 1)$ distribution. The control subjects then have a followup visit from $N(12, 1)$, while the treatment subjects are from $N(17, 1)$.
<!-- One control subject was lost to followup. -->

::: 
::::

## Plot

```{r}
trial |> 
  ggplot(aes(vst, res, color = grp, label = id2)) +
  geom_text()
```


# First Model

## Reshape

A simple way to handle this data is to just model the difference between visit 1 and visit 2. To do that, we need to pivot. Notice I name the difference `res_diff`.

```{r}
trial_wide <- 
  trial |> 
  pivot_wider(names_from = vst, values_from = res) |> 
  mutate(res_diff =  vst2 - vst1)

trial_wide
```

## Plot

```{r}
trial_wide |> 
  ggplot(aes(grp, res_diff, color = grp, label = id2)) +
  geom_text()
```


## First model

Here is the basic syntax for a **l**inear **m**odel. We can save the model to a variable. 

`res_diff ~ grp` says, "predict the mean of the pre-post difference as a function of the group."

```{r}
mod1 <- lm(res_diff ~ grp, data = trial_wide)

mod1
```

In this case, the `(Intercept)` is the reference class, and `grptrt` (`grp` variable with `trt` class) is the "offset".

## First model, plot

```{r}
trial_wide |> 
  ggplot(aes(grp, res_diff, color = grp, label = id2)) +
  geom_text() +
  geom_hline(yintercept = mod1$coefficients, lty = 2)
```

## First model, interpretation

:::: columns

::: {.column width="30%"}
`summary` gives the coefficient estimate error and P-value. Recall that we did the $N(10, 1)$ distribution for `pre`, $N(12, 1)$ for `post` of `crl` and $N(17, 1)$ for `post` of `trt`.

So $12-10 \sim 2$ (`(Intercept)`) and $17-10 + 2 \sim 5$ (`grptrt`). The model recovered the simulation parameters. 
:::
::: {.column width="70%"}

```{r}
summary(mod1)
```

:::

::::

## First model, object

The model object bundles several pieces of our model. Many of these have "accessor" functions, or you can just use the `$` to access them.

```{r}
mod1 |> names()

mod1 |> coef()

mod1$coefficients
```

## Summary

So far, we have one model, one predictor and one coefficient. But often we want to compare many model coefficients or metrics at once. For instance, we could look at the effect of a bunch of blood labs at ICU admission on 30-day mortality. We can do a separate model for each, or treat each as a covariate. In either case, we can use our data manipulation tools to treat *model measures* like *data points*.

# Model measures

## Measures

There are two basic classes of measures associated with a model: 

1. effect estimates
1. fit metrics

## Extract

Each of these can be extracted in a convenient `tibble` form from the model object with the `{broom}` package and the functions `tidy()` and `glance()`. 

```{r}
library(broom)

tidy(mod1)
glance(mod1)
```

We are going to focus on comparing coefficients with `tidy`

# Model, Round 2

## Demographics

Let's make a second model with more simulated data. 

:::: columns

::: column

```{r}
set.seed(125)

id <- paste0("id", seq(1:6))
dob <- 
  sample(seq(as.Date('1999/01/01'),
             as.Date('2000/01/01'),
             by="day"), 6)
dod <- 
  sample(seq(as.Date('2016/01/01'),
             as.Date('2017/01/01'),
             by="day"), 6)
weight <- 
  rnorm(n = 6, mean = 170, sd = 5)

demo <- 
  tibble(id, dob, dod, weight) |> 
  mutate(age = (dod - dob)/dyears(1))

```
:::
::: column
```{r echo=FALSE}
demo |> select(-dod)
```

:::
::::

## Plot

Since the data was random, there is no relationship with each other or the `res_diff` data.

```{r}
demo |> 
  ggplot(aes(age, weight)) +
  geom_point()
```

## Joining

```{r}
trial_joint <- 
  trial_wide |> 
  left_join(demo, by = c("id2" = "id"))

trial_joint
```

## Second model

`mod2` is the same as `mod1` but we are adding the `weight` and `age` covariates from the `demo` data. We know that there is no relationship with the `demo` variables. Often there are many covariate like this we want to test. 

```{r}
mod2 <- lm(res_diff ~ grp + weight + age, data = trial_joint)

tidy(mod2, conf.int = TRUE)
```

Notice that at $\alpha = 0.5$, only `grptrt` has a significant `p.value`

## Coefficient plot

We know from `p.value` which estimates matter. But the plot can help us compare the effect size scale at a glance for very many. 

```{r}
tidy(mod2, conf.int = TRUE) |> 
  filter(term != "(Intercept)") |> 
  ggplot(aes(term, estimate, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point() +
  geom_hline(yintercept = 0, lty = 2) +
  coord_flip()
```


# Supplement: other models

## Interactions

The `:` in a formula is for interactions; the `*` in a formula is for crossing. Notice these two are equivalent

```{r}
lm(res_diff ~ grp + weight + age + weight:age, data = trial_joint)
lm(res_diff ~ grp + weight*age, data = trial_joint)
```

## Powers

The `^x` crosses something with itself `x` many times. For a single continuous variable, this is equivalent to polynomial regression. 

```{r}
lm(res_diff ~ grp + weight^2, data = trial_joint)
lm(res_diff ~ grp + weight + weight^2, data = trial_joint)
```

## Removing

Finally, the `-` in a formula removes that term.

```{r}
lm(res_diff ~ grp + weight*age - weight:age, data = trial_joint)
```



# Model Comparison

## Comparison

Now that we have two models, we can keep everything is organized in a tibble, and we can readily make comparisons.

```{r}
mod1_coef <- tidy(mod1) |> mutate(mod = "mod1")
mod2_coef <- tidy(mod2) |> mutate(mod = "mod2")
coefs <- bind_rows(mod1_coef, mod2_coef) |> relocate(mod)
coefs

coefs |> filter(term == "grptrt")
```

## Comparison 2

Likewise for metrics

```{r}
mod1_met <- glance(mod1) |> mutate(mod = "mod1")
mod2_met <- glance(mod2) |> mutate(mod = "mod2")
mets <- bind_rows(mod1_met, mod2_met) |> relocate(mod)
mets

mets |> select(mod, r.squared, p.value, df)
```

## Presentation

From here, it is very easy to display our models in a variety of formats.

```{r}
mets |> 
  select(mod, r.squared, p.value, df) |> 
  knitr::kable(digits = 3, "html")
```
